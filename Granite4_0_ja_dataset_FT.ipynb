{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kolinz/notebook4AI/blob/main/Granite4_0_ja_dataset_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5mGu8CKUdiB"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + â­ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã“ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚‚ã¨ã«ãªã£ãŸãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Œ[Granite4.0_350M.ipynb](https://github.com/unslothai/notebooks/blob/main/nb/Granite4.0_350M.ipynb)ã€ã§ã™ã€‚"
      ],
      "metadata": {
        "id": "VD8b7kNO_q2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ã‚‹Google ãƒ‰ãƒ©ã‚¤ãƒ–ã«æ¥ç¶šã€‚"
      ],
      "metadata": {
        "id": "aldwBRR0WhgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wqL5b58yUhwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a347c48e-6b64-4296-d162-3d7b9b7b87fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Uolm_XUdiE"
      },
      "source": [
        "### Unslothã¨Mambaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9iogy_kUdiE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, importlib.util\n",
        "!pip install --upgrade -qqq uv\n",
        "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    !uv pip install -qqq \\\n",
        "        \"torch==2.7.1\" \"triton>=3.3.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
        "        \"unsloth-zoo==2025.12.7\" \\\n",
        "        \"unsloth==2025.12.9\"\n",
        "elif importlib.util.find_spec(\"unsloth\") is None:\n",
        "    !uv pip install -qqq unsloth\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo\n",
        "\n",
        "# These are mamba kernels and we must have these for faster training\n",
        "# Mamba kernels are for now supported only on torch==2.7.1. If you have newer torch versions, please wait 30 minutes for it to compile\n",
        "!uv pip install --no-build-isolation mamba_ssm==2.2.5\n",
        "!uv pip install --no-build-isolation causal_conv1d==1.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unslothã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xbb0cuLzwgf"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "print(\"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ™ãƒ¼ã‚¹ã«ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®å–å¾—\")\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/granite-4.0-micro\",\n",
        "    \"unsloth/granite-4.0-h-micro\",\n",
        "    \"unsloth/granite-4.0-h-tiny\",\n",
        "    \"unsloth/granite-4.0-h-small\",\n",
        "\n",
        "    # Base pretrained Granite 4 models\n",
        "    \"unsloth/granite-4.0-micro-base\",\n",
        "    \"unsloth/granite-4.0-h-micro-base\",\n",
        "    \"unsloth/granite-4.0-h-tiny-base\",\n",
        "    \"unsloth/granite-4.0-h-small-base\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/granite-4.0-350m-bnb-4bit\",\n",
        "    max_seq_length = 2048,   # Choose any for long context!\n",
        "    load_in_4bit = False,    # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False,    # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                      \"shared_mlp.input_linear\", \"shared_mlp.output_linear\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
        "#### ğŸ“„ Googleãƒ‰ãƒ©ã‚¤ãƒ–ä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "\n",
        "ãŠå®¢æ§˜ã®å•é¡Œã‚’ç©æ¥µçš„ã«ã‚µãƒãƒ¼ãƒˆã—ã€å•é¡Œè§£æ±ºã‚’è¡Œã†ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã‚µãƒ³ãƒ—ãƒ«ã¯ã€ä»¥ä¸‹ã®2ã¤ã®åˆ—ã‚’æŒã¤CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ç”¨ã„ã¾ã™ã€‚\n",
        "\n",
        "- **input**: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã¨ã®çŸ­ã„ã‚„ã‚Šå–ã‚Š\n",
        "- **recommendation**: å•é¡Œã®è§£æ±ºæ–¹æ³•ã«é–¢ã™ã‚‹ææ¡ˆ\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "granite-4 ã®ãƒãƒ£ãƒƒãƒˆ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
        "```\n",
        "<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\n",
        "Today's Date: June 24, 2025.\n",
        "You are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\n",
        "\n",
        "<|start_of_role|>user<|end_of_role|>How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|end_of_text|>\n",
        "\n",
        "<|start_of_role|>assistant<|end_of_role|>Astronomers make use of the unique spectral fingerprints of elements found in stars...<|end_of_text|>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã‚’å®Ÿæ–½"
      ],
      "metadata": {
        "id": "7kwlARUixT1O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkq4RvEq7FQr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. CSVã®èª­ã¿è¾¼ã¿\n",
        "sheet_path = '/content/drive/MyDrive/finetune_support_ja_100_rich.csv'\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"train\": sheet_path},\n",
        "    column_names=[\"input\", \"recommendation\"],\n",
        "    skiprows=1\n",
        ")[\"train\"]\n",
        "\n",
        "# 2. ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹é–¢æ•°\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs  = examples[\"input\"]\n",
        "    outputs = examples[\"recommendation\"]\n",
        "    texts = []\n",
        "    for input_text, output_text in zip(inputs, outputs):\n",
        "        # Granite Baseãƒ¢ãƒ‡ãƒ«ãŒã€Œå¯¾è©±ã€ã¨ã—ã¦èªè­˜ã§ãã‚‹ã‚¿ã‚°ã‚’ä»˜ã‘ã¾ã™\n",
        "        # inputè‡ªä½“ã« \"User:\" ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚’æ´»ã‹ã—ã¤ã¤æ§‹æˆã—ã¾ã™\n",
        "        text = f\"<|user|>\\n{input_text}\\n<|assistant|>\\n{output_text}\\n<|endoftext|>\"\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "csv å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦èª­ã¿è¾¼ã¿ã¾ã—ãŸãŒã€ä»¥ä¸‹ã®ã‚ˆã†ã«ä¼šè©±å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€ãƒãƒ£ãƒƒãƒˆ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```\n",
        "\n",
        "helper function `formatting_prompts_func` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reoBXmAn7HlN"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    user_texts = examples['input']\n",
        "    response_texts = examples['recommendation']\n",
        "    messages = [\n",
        "        [{\"role\": \"user\", \"content\": user_text},\n",
        "        {\"role\": \"assistant\", \"content\": response_text}] for user_text, response_text in zip(user_texts, response_texts)\n",
        "    ]\n",
        "    texts = [tokenizer.apply_chat_template(message, tokenize = False, add_generation_prompt = False) for message in messages]\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Explicitly set the chat template for the tokenizer\n",
        "DEFAULT_CHAT_TEMPLATE = \"\"\"{% set system_message = 'You are a helpful assistant. Please ensure responses are professional, accurate, and safe.' %}\n",
        "{% set loop_messages = messages %}\n",
        "\n",
        "{% if messages %}\n",
        "    {% if messages[0]['role'] == 'system' %}\n",
        "        {% set loop_messages = messages[1:] %}\n",
        "        {% set system_message = messages[0]['content'] %}\n",
        "    {% endif %}\n",
        "{% endif %}\n",
        "\n",
        "{{ '<|start_of_role|>system<|end_of_role|>' + system_message + '<|end_of_text|>' }}\n",
        "{% for message in loop_messages %}\n",
        "    {{ '<|start_of_role|>' + message['role'] + '<|end_of_role|>' + message['content'] + '<|end_of_text|>' }}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "    {{ '<|start_of_role|>assistant<|end_of_role|>' }}\n",
        "{% endif %}\"\"\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5Sx9In7vHi"
      },
      "source": [
        "ã“ã“ã§ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‰ã®ç”Ÿã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzE1OEXi7s3P"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"input\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA-aC7w-x72-"
      },
      "outputs": [],
      "source": [
        "dataset[5]['recommendation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q76PqXv9yKa0"
      },
      "source": [
        "ãƒãƒ£ãƒƒãƒˆ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã“ã‚Œã‚‰ã®ä¼šè©±ã‚’ã©ã®ã‚ˆã†ã«å¤‰ãˆãŸã‹ãŒã‚ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du1MB2NqyGW4"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚é«˜é€ŸåŒ–ã®ãŸã‚60ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè¡Œã—ã¾ã™ãŒã€ãƒ•ãƒ«å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€Œnum_train_epochs=1ã€ã«è¨­å®šã—ã€ã€Œmax_steps=Noneã€ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
        "\n",
        "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚¹ãŒã€nan ã«ãªã‚‹å ´åˆã¯ã€ã“ã¡ã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã€ãƒ™ãƒ¼ã‚¹ã«ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã€å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã®ï¼“ã¤ã‚’è¦‹ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "import torch # Make sure torch is imported to use torch.cuda.is_bf16_supported\n",
        "\n",
        "# Removed 'from unsloth.util import is_bfloat16_supported' as it's deprecated/removed.\n",
        "# Using torch.cuda.is_bf16_supported() directly.\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        max_grad_norm = 0.5,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_torch\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(), # A100/30ç³»ãªã‚‰çµ¶å¯¾bf16ãŒå®‰å®š\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "Unslothã®train_on_completionsãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å‡ºåŠ›ã®ã¿ã§å­¦ç¿’ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«ã‚ˆã‚‹æå¤±ã‚’ç„¡è¦–ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã®ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_of_role|>user|end_of_role|>\",\n",
        "    response_part = \"<|start_of_role|>assistant<|end_of_role|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "å‘½ä»¤éƒ¨åˆ†ã®ãƒã‚¹ã‚¯ãŒã§ããŸã“ã¨ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼ã‚‚ã†ä¸€åº¦5è¡Œç›®ã‚’å‡ºåŠ›ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtsMVtlkUhja"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kyjy__m9KY3"
      },
      "source": [
        "æ¬¡ã«ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸä¾‹ã‚’å‡ºåŠ›ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ç­”ãˆã ã‘ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rD6fl8EUxnG"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ã‚‡ã†ï¼ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚’å†é–‹ã™ã‚‹ã«ã¯ã€trainer.train(resume_from_checkpoint = True) ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
        "\n",
        "```\n",
        "Mambaã‚«ãƒ¼ãƒãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã¯ç´„10åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã®ã§ã”æ³¨æ„ãã ã•ã„ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"æ¨è«–\"></a>\n",
        "### æ¨è«–\n",
        "Unslothãƒã‚¤ãƒ†ã‚£ãƒ–æ¨è«–ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼å­¦ç¿’å†…å®¹ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ã‚’ã„ãã¤ã‹ä½¿ç”¨ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk4A3U4l-SjD"
      },
      "outputs": [],
      "source": [
        "# @title Test Scenarios\n",
        "# --- Scenario 1: Video-Conferencing Screen-Share Bug (11 turns) ---\n",
        "scenario_1 = \"\"\"\n",
        "User: ä¼šè­°ã®å‚åŠ è€…å…¨å“¡ã€ç§ãŒç”»é¢å…±æœ‰ã‚’ã™ã‚‹ã¨çœŸã£æš—ãªç”»é¢ã—ã‹è¦‹ãˆãªã„ã‚ˆã†ã§ã™ã€‚\n",
        "Agent: ã”ä¸ä¾¿ã‚’ãŠã‹ã‘ã—ã¦ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ã‹ã€ãã‚Œã¨ã‚‚ç”»é¢å…¨ä½“ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
        "User: macOS Sonomaã§ç”»é¢å…¨ä½“ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ã€‚\n",
        "Agent: ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã€Œè¨­å®šã€â†’ã€Œãƒ“ãƒ‡ã‚ªã€ã«ã‚ã‚‹ã€Œãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã€ã®ã‚¹ã‚¤ãƒƒãƒã¯ã‚ªãƒ³ã«ãªã£ã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
        "User: ã¯ã„ã€ãã®ã‚¹ã‚¤ãƒƒãƒã¯ã‚ªãƒ³ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "Agent: ä¸€åº¦ãã‚Œã‚’ã‚ªãƒ•ã«åˆ‡ã‚Šæ›¿ãˆã¦ã‹ã‚‰ã€ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå…±æœ‰ã‚’è©¦ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n",
        "User: è©¦ã—ã¾ã—ãŸãŒã€å‚åŠ è€…ã«ã¯ã¾ã çœŸã£æš—ã«è¦‹ãˆã‚‹ã‚ˆã†ã§ã™ã€‚\n",
        "Agent: æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªã®v5.4.2ã‚’ãŠä½¿ã„ã§ã™ã‹ã€ãã‚Œã¨ã‚‚ãƒ–ãƒ©ã‚¦ã‚¶ç‰ˆã‚’ã”åˆ©ç”¨ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
        "User: ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ç‰ˆã®v5.4.2ã§ã™ã€‚ã¡ã‚‡ã†ã©ä»Šæœã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã—ãŸã°ã‹ã‚Šã§ã™ã€‚\n",
        "\"\"\"\n",
        "\n",
        "# --- Scenario 2: Smart-Lock Low-Battery Loop (9 turns) ---\n",
        "scenario_2 = \"\"\"\n",
        "User: é›»æ± ã‚’äº¤æ›ã—ãŸã®ã§ã™ãŒã€ãƒ­ãƒƒã‚¯ã‚¢ãƒ—ãƒªã®è¡¨ç¤ºãŒ5%ã®ã¾ã¾ã§ã€ã‚ªãƒ¼ãƒˆãƒ­ãƒƒã‚¯ãŒä½œå‹•ã—ã¾ã›ã‚“ã€‚\n",
        "Agent: ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚ã‚¢ãƒ—ãƒªã®ã€Œè¨­å®šã€â†’ã€Œãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã€ã«é€²ã¿ã€ã©ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n",
        "User: 3.18.0-alphaã§ã™ã€‚\n",
        "Agent: æœ€æ–°ã®å®‰å®šç‰ˆã¯3.17.5ã§ã™ã€‚ãƒ™ãƒ¼ã‚¿ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
        "User: æ•°ãƒ¶æœˆå‰ã«ç™»éŒ²ã—ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n",
        "Agent: ãƒ™ãƒ¼ã‚¿ç‰ˆã®ãƒ“ãƒ«ãƒ‰ã§ã¯ã€ç¨€ã«ãƒãƒƒãƒ†ãƒªãƒ¼æ®‹é‡ãŒèª¤å ±ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚é›»æ± ã‚’ä¸€åº¦æŠœãã€10ç§’å¾…ã£ã¦ã‹ã‚‰å†æŒ¿å…¥ã—ã¦ã€LEDã®ç‚¹æ»…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "User: LEDãŒé’ã«2å›ç‚¹æ»…ã—ãŸå¾Œã€èµ¤ã«1å›ç‚¹æ»…ã—ã¾ã—ãŸã€‚\n",
        "Agent: ãã®ç‚¹æ»…ã‚³ãƒ¼ãƒ‰ã¯ã€Œè¨­å®šã®ä¸ä¸€è‡´ï¼ˆconfig mismatchï¼‰ã€ã‚’æ„å‘³ã—ã¾ã™ã€‚ãŠæ‰‹å…ƒã«å¤ã„æ–¹ã®é›»æ± ã¯ã¾ã æ®‹ã£ã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
        "User: ã‚‚ã†æ¨ã¦ã¦ã—ã¾ã„ã¾ã—ãŸã€‚\n",
        "\"\"\"\n",
        "\n",
        "# --- Scenario 3: Accounting SaaS â€” Corrupted Invoice Export (10 turns) ---\n",
        "scenario_3 = \"\"\"\n",
        "User: ä»Šæ—¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹è«‹æ±‚æ›¸ãŒã€ã©ã‚Œã‚‚ç™½ç´™ã®PDFã¨ã—ã¦é–‹ã„ã¦ã—ã¾ã„ã¾ã™ã€‚\n",
        "Agent: ãã‚Œã¯éå»ã®è«‹æ±‚æ›¸ã€æ–°ã—ã„ã‚‚ã®ã€ã‚ã‚‹ã„ã¯ä¸¡æ–¹ã§ç™ºç”Ÿã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
        "User: ä¸¡æ–¹ã§ã™ã€‚æ›¸ãå‡ºã—ãŸã‚‚ã®ã¯ã™ã¹ã¦0ãƒã‚¤ãƒˆã«ãªã‚Šã¾ã™ã€‚\n",
        "Agent: ã€Œä¸€æ‹¬æ“ä½œã€ã‹ã‚‰æ›¸ãå‡ºã—ã¦ã„ã¾ã™ã‹ã€ãã‚Œã¨ã‚‚å€‹åˆ¥ã®è«‹æ±‚æ›¸ãƒšãƒ¼ã‚¸ã‹ã‚‰ã§ã™ã‹ï¼Ÿ\n",
        "User: å€‹åˆ¥ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã§ã™ã€‚\n",
        "Agent: ãƒ–ãƒ©ã‚¦ã‚¶ã¨OSã®çµ„ã¿åˆã‚ã›ã¯ä½•ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
        "User: Windows 11ä¸Šã®Chromeã§ã™ã€‚æœ€æ–°ã®çŠ¶æ…‹ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n",
        "Agent: UTCã®åˆå‰10æ™‚ã«æ–°ã—ã„PDFãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¸å…·åˆã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã«ã€ä¸€åº¦Edgeã§è©¦ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n",
        "User: Edgeã§è©¦ã—ã¾ã—ãŸãŒã€ã‚„ã¯ã‚Š0ãƒã‚¤ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ãªã‚Šã¾ã™ã€‚\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": scenario_1},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    padding = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
        "\n",
        "_ = model.generate(**inputs,\n",
        "                   streamer = text_streamer,\n",
        "                   max_new_tokens = 512, # Increase if tokens are getting cut off\n",
        "                   use_cache = True,\n",
        "                   # Adjust the sampling params to your preference\n",
        "                   do_sample=True,\n",
        "                   temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGSmEnAd-sOP"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": scenario_2},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    padding = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
        "\n",
        "_ = model.generate(**inputs,\n",
        "                   streamer = text_streamer,\n",
        "                   max_new_tokens = 512, # Increase if tokens are getting cut off\n",
        "                   use_cache = True,\n",
        "                   # Adjust the sampling params to your preference\n",
        "                   do_sample=False,\n",
        "                   temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã«ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¿å­˜ã®å ´åˆã¯Huggingfaceã®`push_to_hub`ã€ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ã®å ´åˆã¯`save_pretrained`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "**[æ³¨æ„]** ã“ã‚Œã¯LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜ã—ã€å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã—ã¾ã›ã‚“ã€‚16ãƒ“ãƒƒãƒˆã¾ãŸã¯GGUFå½¢å¼ã§ä¿å­˜ã™ã‚‹ã«ã¯ã€ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"granite4_ft_model\")  # Local saving , ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«å lora_model\n",
        "tokenizer.save_pretrained(\"granite4_ft_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "æ¨è«–ç”¨ã«ä¿å­˜ã—ãŸ LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ã€False ã‚’ True ã«è¨­å®šã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"granite4_ft_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": scenario_3},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    padding = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
        "\n",
        "_ = model.generate(**inputs,\n",
        "                   streamer = text_streamer,\n",
        "                   max_new_tokens = 512, # Increase if tokens are getting cut off\n",
        "                   use_cache = True,\n",
        "                   # Adjust the sampling params to your preference\n",
        "                   do_sample=False,\n",
        "                   temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "UCVBLyZKMSd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’æŒ‡å®š\n",
        "if True:\n",
        "    model.save_pretrained_gguf(\"export_model\", tokenizer, quantization_method = \"q8_0\")"
      ],
      "metadata": {
        "id": "shaWUOF3h6o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ¢ãƒ‡ãƒ«åã®å¤‰æ›´ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "\n",
        "ã€Œgranite-4.0-350m.Q8_0.ggufã€ã¨ã—ã¦ã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã“ã¨ãŒã‚ã‹ã‚Šã‚„ã™ã„ã‚ˆã†ã«ã€ã€Œgranite-4.0-350m-FT-ja.Q8_0.ggufã€ã«ãƒ¢ãƒ‡ãƒ«åã‚’å¤‰æ›´ã—ã€Google Colabã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "nSSJ4a9DQJm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "src = \"/content/granite-4.0-350m.Q8_0.gguf\"\n",
        "dst = \"/content/granite-4.0-350m-FT-ja.Q8_0.gguf\"\n",
        "\n",
        "os.rename(src, dst)"
      ],
      "metadata": {
        "id": "OFUfmv_-Tavf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/granite-4.0-350m-FT-ja.Q8_0.gguf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qOtZ6cT-QIh3",
        "outputId": "61ce5a53-7431-400f-ab4a-8c9e95f6061c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c39e012f-2508-4f83-9b51-9a06668c05a6\", \"granite-4.0-350m-FT-ja.Q8_0.gguf\", 378132000)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}